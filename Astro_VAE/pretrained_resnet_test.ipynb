{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de506d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "def get_modified_resnet(in_channels=4, model_name='resnet18'):\n",
    "    \"\"\"Loads a pre-trained ResNet and modifies the first conv layer for 4 input channels.\"\"\"\n",
    "    \n",
    "    # 1. Load the pre-trained model weights\n",
    "    # We use ResNet18_Weights.IMAGENET1K_V1 which contains ImageNet weights\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Get the existing Conv1 layer (which takes 3 input channels)\n",
    "    original_conv1 = model.conv1\n",
    "    \n",
    "    # 2. Create a new Conv1 layer for 4 input channels\n",
    "    # Keep all other parameters (out_channels=64, kernel_size, stride, padding, bias) the same\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels, \n",
    "        original_conv1.out_channels, \n",
    "        kernel_size=original_conv1.kernel_size, \n",
    "        stride=original_conv1.stride, \n",
    "        padding=original_conv1.padding, \n",
    "        bias=original_conv1.bias\n",
    "    )\n",
    "    \n",
    "    # 3. Transfer the pre-trained weights to the new layer\n",
    "    # Since the new layer has 4 channels and the old one has 3, we copy the weights\n",
    "    # for the first 3 channels and initialize the 4th channel's weights randomly (or to zeros).\n",
    "    # We copy the weights using the first 3 channels of the original weights.\n",
    "    new_conv1.weight.data[:, :3, :, :] = original_conv1.weight.data\n",
    "    \n",
    "    # Initialize the new 4th channel's weights (e.g., by averaging the first 3 or setting to zero)\n",
    "    # Using the mean of the first three channels is a common heuristic:\n",
    "    new_conv1.weight.data[:, 3, :, :] = original_conv1.weight.data[:, :3, :, :].mean(dim=1)\n",
    "\n",
    "    # 4. Replace the original layer with the new one\n",
    "    model.conv1 = new_conv1\n",
    "    \n",
    "    # 5. Modify the final layer (Fully Connected) to output the desired embedding size\n",
    "    # We cut off the classification head and just use the feature extractor backbone.\n",
    "    # The output size of the backbone is typically 512 for ResNet18.\n",
    "    model = nn.Sequential(*list(model.children())[:-1]) # Remove the classification layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db2e539",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMAGE_CHANNELS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Load your existing data_loader (64x64 patches, 4 channels)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# data_loader = get_dataloader(...) \u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# --- 1. Load and Modify Foundation Model ---\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m get_modified_resnet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[43mIMAGE_CHANNELS\u001b[49m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet-18 model modified and loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# --- 2. Extract Embeddings ---\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# features_Z_VAE = extract_features(vae_model, data_loader, device) # Your VAE embeddings\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IMAGE_CHANNELS' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have a working get_dataloader function and device setup\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_foundation_features(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extracts features using the modified pre-trained ResNet.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Extracting ResNet Embeddings\", unit=\"batch\")\n",
    "    \n",
    "    for x in pbar:\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = x[0]\n",
    "            \n",
    "        x = x.to(torch.float32).to(device)\n",
    "        \n",
    "        # Forward pass through the ResNet backbone\n",
    "        features = model(x)\n",
    "        \n",
    "        # Features will be (Batch, 512, 1, 1). We squeeze out the 1x1 dimensions.\n",
    "        features = features.squeeze()\n",
    "        \n",
    "        all_features.append(features.cpu())\n",
    "        \n",
    "    return torch.cat(all_features, dim=0).numpy()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Setup ---\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load your existing data_loader (64x64 patches, 4 channels)\n",
    "    # data_loader = get_dataloader(...) \n",
    "    \n",
    "    # --- 1. Load and Modify Foundation Model ---\n",
    "    foundation_model = get_modified_resnet(in_channels=IMAGE_CHANNELS).to(torch.float32).to(device)\n",
    "    print(\"ResNet-18 model modified and loaded successfully.\")\n",
    "\n",
    "    # --- 2. Extract Embeddings ---\n",
    "    # features_Z_VAE = extract_features(vae_model, data_loader, device) # Your VAE embeddings\n",
    "    features_Z_ResNet = extract_foundation_features(foundation_model, data_loader, device)\n",
    "    \n",
    "    # --- 3. Comparison ---\n",
    "    print(f\"\\nVAE Latent Dim (Example): 128\")\n",
    "    print(f\"ResNet Embedding Dim: {features_Z_ResNet.shape[1]}\")\n",
    "    \n",
    "    # Now you can compare the two sets of embeddings:\n",
    "    # 1. Compare Clustering: Apply UMAP/t-SNE to both VAE and ResNet embeddings.\n",
    "    #    (You may need to reduce the ResNet features from 512D to 128D first, \n",
    "    #     or let UMAP handle the reduction.)\n",
    "    # 2. Compare Performance: Use the embeddings as input for a simple linear classifier \n",
    "    #    to predict a known property (e.g., redshift, if you have labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b2ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
